!pip install evaluate datasets accelerate torch huggingface_hub
!pip install git+https://github.com/huggingface/transformers 

!python run_clm.py --model_type gpt2 --tokenizer_name openai-community/gpt2 --dataset_name="dustinwloring1988/fineweb-small-test-sample" --output_dir="/output" --config_overrides="n_embd=1024,n_head=8,n_layer=8,n_positions=1024"

or

!python run_clm.py --model_type gpt2 --tokenizer_name dustinwloring1988/llama3-tokenizer --dataset_name="dustinwloring1988/fineweb-small-test-sample" --bf16=False --fp16=True --gradient_checkpointing=False --save_steps=2000 --learning_rate=4e-4 --push_to_hub=True --hub_model_id="dustinwloring1988/test-upload" --push_to_hub_token="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" --weight_decay=0.01 --warmup_steps=128 --logging_steps=5 --per_device_train_batch_size=8 --per_device_eval_batch_size=8 --gradient_accumulation_steps=16 --output_dir="/output" --config_overrides="n_embd=1024,n_head=8,n_layer=8,n_positions=1024,scale_attn_by_inverse_layer_idx=True,reorder_and_upcast_attn=True,bos_token_id=128000,eos_token_id=128001"

or

python -m torch.distributed.launch --nproc_per_node=#NUM_GPUS_YOU_HAVE
YOUR_TRAINING_SCRIPT.py (â€“arg1 --arg2 --arg3 and all other arguments of your training script)

or

!python run_fim.py \
    --model_name_or_path gpt2 \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --fim_rate 0.5 \
    --fim_spm_rate 0.2 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm
